{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Track the datasets in your Azure ML experiment using Python SDK v2 and MLflow #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Prerequisites to run this notebook:   \n",
        "insatll Python SDKv2: [InstallSDKv2.ipynb](/Users/hjilova/AzureML/TrackingDatasetsMLflow/InstallSDKv2.ipynb) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Ensure you have the dependencies for this notebook\n",
        "%pip install -r logging_datasets_with_mlflow.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SDK version: 1.44.0\n",
            "MLflow version: 1.28.0\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "import mlflow\n",
        "import mlflow.azureml\n",
        "\n",
        "import azureml.core\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from azureml.core import Dataset\n",
        "#from azureml.data.dataset_factory import DataType\n",
        "import tempfile\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n",
        "print(\"MLflow version:\", mlflow.version.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: TrackDatasets\n",
            "Experiment_id: 475b58cf-0796-4959-8c22-2d90e18889ec\n"
          ]
        }
      ],
      "source": [
        "experiment_name =\"TrackDatasets\"\n",
        "experiment=mlflow.set_experiment(experiment_name)\n",
        "experiment_id=experiment.experiment_id\n",
        "\n",
        "# Create an experiment with a name that is unique and case sensitive.\n",
        "client = MlflowClient()\n",
        "#experiment_id = client.create_experiment(experiment_name)\n",
        "client.set_experiment_tag(experiment_id, \"exper ver\", \"1\")\n",
        "\n",
        "\n",
        "# Fetch experiment metadata information\n",
        "experiment = client.get_experiment(experiment_id)\n",
        "print(\"Name: {}\".format(experiment.name))\n",
        "print(\"Experiment_id: {}\".format(experiment.experiment_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#import the dataset\n",
        "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
        "df = pd.read_csv(file_url)\n",
        "df[\"thal\"] = df[\"thal\"].astype(\"category\").cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(\"target\", axis=1), df[\"target\"], test_size=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "There are two main approaches to track the datasets:   \n",
        "The first one is to save a copy of your input data:\n",
        "\n",
        "```\n",
        "    with tempfile.TemporaryDirectory() as tmp:\n",
        "        path = 'saveDatasetVer/train_dataset.csv'  #path where you whant to save your dataset used for training \n",
        "        X_train.to_csv(path)\n",
        "        mlflow.log_artifacts(tmp)\n",
        "```\n",
        "This will save your input dataset as a csv artifact.\n",
        "\n",
        "In case the dataset is too big, you can log the path to the dataset as a parameter: \n",
        "\n",
        "```\n",
        "    mlflow.log_param('dsPathМ', file_url )   #track the pointer to the data\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Registered model 'DatastetsModel' already exists. Creating a new version of this model...\n",
            "2022/10/04 12:29:35 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: DatastetsModel, version 6\n",
            "Created version '6' of model 'DatastetsModel'.\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "with mlflow.start_run():  \n",
        "    mlflow.xgboost.autolog(log_models=True,log_input_examples=True,log_model_signatures=True,registered_model_name='DatastetsModel')\n",
        "\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric=\"auc\")\n",
        "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "\n",
        "    mlflow.log_param('dsPathМ', file_url )   #track the pointer to the data\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp:\n",
        "        path = 'saveDatasetVer/train_dataset.csv'  #path where you whant to save your dataset used for training \n",
        "        X_train.to_csv(path)\n",
        "        mlflow.log_artifacts(tmp)\n",
        "\n",
        "    signature = infer_signature(X_test, y_test)\n",
        "    mlflow.xgboost.log_model(model, \"classifier\", signature=signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "additional info:   \n",
        "**log_models** -  If True, trained models are logged as MLflow model artifacts. If False, trained models are not logged. Input examples and model signatures, which are attributes of MLflow models, are also omitted when log_models is False.  \n",
        "\n",
        "\n",
        "**registered_model_name** – If given, each time a model is trained, it is registered as a new model version of the registered model with this name. The registered model is created if it does not already exist.    \n",
        "\n",
        "**log_input_examples** – If True, input examples from training datasets are collected and logged along with XGBoost model artifacts during training. If False, input examples are not logged. Note: Input examples are MLflow model attributes and are only collected if log_models is also True\n",
        "\n",
        "**log_model_signatures** – If True, ModelSignatures describing model inputs and outputs are collected and logged along with XGBoost model artifacts during training. If False, signatures are not logged. Note: Model signatures are MLflow model attributes and are only collected if log_models is also True.\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
