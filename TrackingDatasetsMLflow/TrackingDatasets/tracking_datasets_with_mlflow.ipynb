{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Prerequisites to run this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Track the datasets in your Azure ML experiment using Python SDKv2 and MLflow ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "There are two main approaches to track the datasets: the **first** one is to save a copy of your input data:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Prerequisites to run this notebook:   \n",
        "insatll Python SDKv2: InstallSDKv2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Ensure you have the dependencies for this notebook\n",
        "#%pip install -r logging_model_with_mlflow.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SDK version: 1.44.0\n",
            "MLflow version: 1.28.0\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "import mlflow\n",
        "import mlflow.azureml\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from azureml.core import Dataset\n",
        "#from azureml.data.dataset_factory import DataType\n",
        "import tempfile\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n",
        "print(\"MLflow version:\", mlflow.version.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: TrackDatasets\n",
            "Experiment_id: 1ae4018f-adf2-4b54-ae2c-ad04fe6ddca0\n",
            "Artifact Location: \n",
            "Tags: {'exper ver': '1'}\n",
            "Lifecycle_stage: active\n"
          ]
        }
      ],
      "source": [
        "experiment_name =\"TrackDatasets\"\n",
        "experiment=mlflow.set_experiment(experiment_name)\n",
        "experiment_id=experiment.experiment_id\n",
        "\n",
        "# Create an experiment with a name that is unique and case sensitive.\n",
        "client = MlflowClient()\n",
        "#experiment_id = client.create_experiment(experiment_name)\n",
        "client.set_experiment_tag(experiment_id, \"exper ver\", \"1\")\n",
        "\n",
        "\n",
        "# Fetch experiment metadata information\n",
        "experiment = client.get_experiment(experiment_id)\n",
        "print(\"Name: {}\".format(experiment.name))\n",
        "print(\"Experiment_id: {}\".format(experiment.experiment_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
        "df = pd.read_csv(file_url)\n",
        "df[\"thal\"] = df[\"thal\"].astype(\"category\").cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0c42457f54eff24857ffb448e9ece515\n"
          ]
        }
      ],
      "source": [
        "#Generate Hash from the dataframe \n",
        "import joblib\n",
        "HFile=joblib.hash(df)\n",
        "print(HFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The file located at the path ./saveDatasetVer/train_dataset.csv was last modified at 2022-10-04 11:03:21\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "path = r\"./saveDatasetVer/train_dataset.csv\"\n",
        "\n",
        "ti_m = os.path.getmtime(path)\n",
        " \n",
        "m_ti = time.ctime(ti_m)\n",
        " \n",
        "# Using the timestamp string to create a\n",
        "# time object/structure\n",
        "t_obj = time.strptime(m_ti)\n",
        " \n",
        "# Transforming the time object to a timestamp\n",
        "# of ISO 8601 format\n",
        "TStamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", t_obj)\n",
        " \n",
        "FName=os.path.basename(path)\n",
        "\n",
        "print(f\"The file located at the path {path} was last modified at {T_stamp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('train_dataset.csv',\n",
              " '2022-10-04 11:03:21',\n",
              " '0c42457f54eff24857ffb448e9ece515')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FName, TStamp, HFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(\"target\", axis=1), df[\"target\"], test_size=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "There are two main approaches to track the datasets: the first one is to save a copy of your input data:\n",
        "\n",
        "```\n",
        "    with tempfile.TemporaryDirectory() as tmp:\n",
        "        path = 'saveDatasetVer/train_dataset.csv'  #path where you whant to save your dataset used for training \n",
        "        X_train.to_csv(path)\n",
        "        mlflow.log_artifacts(tmp)\n",
        "```\n",
        "\n",
        "In case the dataset is too big you can log the path to the dataset as a parameter: \n",
        "\n",
        "```\n",
        "    mlflow.log_param('dsPathМ', file_url )   #track the pointer to the data\n",
        "    mlflow.log_param('FileName', FName)  #track the source FileName\n",
        "    mlflow.log_param('LastModified', TStamp) #track the Last Modified date of the source file\n",
        "    mlflow.log_param('HashedData', HFile) #track the  data (hashed data frame)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022/10/05 11:31:29 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "Registered model 'DatastetsModel' already exists. Creating a new version of this model...\n",
            "2022/10/05 11:31:37 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: DatastetsModel, version 7\n",
            "Created version '7' of model 'DatastetsModel'.\n",
            "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/mlflow/models/signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  outputs = _infer_schema(model_output) if model_output is not None else None\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "with mlflow.start_run():  \n",
        "    mlflow.xgboost.autolog(log_models=True,log_input_examples=True,log_model_signatures=True,registered_model_name='DatastetsModel')\n",
        "\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric=\"auc\")\n",
        "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "\n",
        "    mlflow.log_param('dsPathМ', file_url )   #track the pointer to the data\n",
        "    mlflow.log_param('FileName', FName)   #track the source FileName\n",
        "    mlflow.log_param('LastModified', TStamp) #track the Last Modified date of the source file\n",
        "    mlflow.log_param('HashedData', HFile) #track the  data (hashed data frame)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp:\n",
        "        path = 'saveDatasetVer/train_dataset.csv'  #path where you whant to save your dataset used for training \n",
        "        X_train.to_csv(path)\n",
        "        mlflow.log_artifacts(tmp)\n",
        "\n",
        "    signature = infer_signature(X_test, y_test)\n",
        "    mlflow.xgboost.log_model(model, \"classifier\", signature=signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**log_models** -  If True, trained models are logged as MLflow model artifacts. If False, trained models are not logged. Input examples and model signatures, which are attributes of MLflow models, are also omitted when log_models is False.  \n",
        "\n",
        "\n",
        "**registered_model_name** – If given, each time a model is trained, it is registered as a new model version of the registered model with this name. The registered model is created if it does not already exist.    \n",
        "\n",
        "**log_input_examples** – If True, input examples from training datasets are collected and logged along with XGBoost model artifacts during training. If False, input examples are not logged. Note: Input examples are MLflow model attributes and are only collected if log_models is also True\n",
        "\n",
        "**log_model_signatures** – If True, ModelSignatures describing model inputs and outputs are collected and logged along with XGBoost model artifacts during training. If False, signatures are not logged. Note: Model signatures are MLflow model attributes and are only collected if log_models is also True.\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
